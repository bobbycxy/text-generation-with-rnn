{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with RNNs\n",
    "\n",
    "By Bobby Cheng\n",
    "\n",
    "Welcome to my Text Generation Using Sequence Models Notebook.\n",
    "\n",
    "## 1. Introduction - Generating Bible Verses\n",
    "\n",
    "This is a text generation project that uses stateful GRUs to train a character-level language model. This is where we feed text input data into the GRU and make it sample the next character of a sequence with previous characters, based on a probability distribution. In the spirit of making this fun and light hearted, I will be using the dataset of the bible to train a caharacter-level language model which can be used to generate fictional bible verses. \n",
    "\n",
    "Along the way, I'll aim to answer the following questions:\n",
    "- How to process your input data for text generation with GRU?\n",
    "- How to use the Sequential API to build a text generator neural network architecture?\n",
    "- How sensible will the generated texts be?\n",
    "- Can we tune the models to be more conserve or diverse?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Data\n",
    "\n",
    "Our data will be the King James Bible which consists of 66 books. I downloaded the bible dataset from the following github [here](https://raw.githubusercontent.com/mxw/grmr/master/src/finaltests/bible.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 4451368 characters\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('bible.txt', 'https://raw.githubusercontent.com/mxw/grmr/master/src/finaltests/bible.txt')\n",
    "\n",
    "## read the path_to_file\n",
    "text = open(path_to_file, 'rb').read().decode(encoding = 'utf-8') \n",
    "print(f'Length of text: {len(text)} characters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:1 In the beginning God created the heaven and the earth.\n",
      "\n",
      "1:2 And the earth was without form, and void; and darkness was upon\n",
      "the face of the deep. And the Spirit of God moved upon the face of the\n",
      "waters.\n",
      "\n",
      "1:3 And God said, Let there be light: and there was light.\n",
      "\n",
      "1:4 And God saw the light, that it was good: and God divided the light\n",
      "from the darkness.\n",
      "\n",
      "1:5 And God called the light Day, and the darkness he called Night.\n",
      "And the evening and the morning were the first day.\n",
      "\n",
      "1:6 An\n"
     ]
    }
   ],
   "source": [
    "## Print the first 500 characters in the text\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 unique characters\n"
     ]
    }
   ],
   "source": [
    "## The number of unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "### 3.1. Vectorize our text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'T', b'h', b'i', b's', b' ', b'i', b's', b' ', b'a', b' ', b't', b'e',\n",
       "  b's', b't']                                                            ,\n",
       " [b'g', b'o', b't', b'c', b'h', b'a']]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Since our model is a character level prediction language model,\n",
    "## we'll want to convert our inputs from words into characters.\n",
    "## For that, we can use tf.strings.unicode_split. \n",
    "## The output will be ragged tensor object\n",
    "\n",
    "sample_texts = ['This is a test', 'gotcha']\n",
    "\n",
    "chars = tf.strings.unicode_split(sample_texts, input_encoding = 'UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[49, 63, 64, 74, 3, 64, 74, 3, 56, 3, 75, 60, 74, 75],\n",
       " [62, 70, 75, 58, 63, 56]]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a StringLookup layer that will convert a string of characters into numerical IDs.\n",
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary = list(vocab), mask_token = None)\n",
    "\n",
    "## illustrate how ids_from_chars will convert 'chars' into integers\n",
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'T', b'h', b'i', b's', b' ', b'i', b's', b' ', b'a', b' ', b't', b'e',\n",
       "  b's', b't']                                                            ,\n",
       " [b'g', b'o', b't', b'c', b'h', b'a']]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a StringLookup later that will convert the string of numerical IDs into characters\n",
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary = ids_from_chars.get_vocabulary(), invert = True, mask_token = None)\n",
    "\n",
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'This is a test', b'gotcha'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## with the above reinstated chars value, we can reconvert it back to a single string\n",
    "tf.strings.reduce_join(chars, axis=-1).numpy()\n",
    "\n",
    "## Note that 'chars' is a tensor object. Hence, it cannot be joined back with ''.join(<list>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'This is a test', b'gotcha'], dtype=object)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we can create a useful function that would form ragged tensor objects of integers back into fully joined words.\n",
    "def ids_to_text(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
    "\n",
    "ids_to_text(ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Create Training Examples and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4451368\n"
     ]
    }
   ],
   "source": [
    "## Let us now convert the entire text document into numerical ids!\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "print(len(all_ids))\n",
    "\n",
    "## we should see the length as equivalent to the length of characters in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      ":\n",
      "1\n",
      " \n",
      "I\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "b\n",
      "e\n",
      "g\n",
      "i\n",
      "n\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "G\n",
      "o\n",
      "d\n",
      " \n",
      "c\n",
      "r\n",
      "e\n",
      "a\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "## We'll now create a dataset tensor object that will aid us in our text generation operations\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "## The following will illustrate what the dataset tensor object can do.\n",
    "for ids in ids_dataset.take(30):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[17 26 17  3 38 69  3 75 63 60  3 57 60 62 64 69 69 64 69 62  3 36 70 59\n",
      "  3 58 73 60 56 75 60 59  3 75 63 60  3 63 60 56 77 60 69  3 56 69 59  3\n",
      " 75 63 60  3 60 56 73 75 63 14  2  1  2  1 17 26 18  3 30 69 59  3 75 63\n",
      " 60  3 60 56 73 75 63  3 78 56 74  3 78 64 75 63 70 76 75  3 61 70 73 68\n",
      " 12  3 56 69 59], shape=(101,), dtype=int64)\n",
      "---------------------------------------\n",
      "Observe the conversion from ids to text:\n",
      "b'1:1 In the beginning God created the heaven and the earth.\\r\\n\\r\\n1:2 And the earth was without form, and'\n",
      "---------------------------------------\n",
      "\n",
      "tf.Tensor(\n",
      "[ 3 77 70 64 59 27  3 56 69 59  3 59 56 73 66 69 60 74 74  3 78 56 74  3\n",
      " 76 71 70 69  2  1 75 63 60  3 61 56 58 60  3 70 61  3 75 63 60  3 59 60\n",
      " 60 71 14  3 30 69 59  3 75 63 60  3 48 71 64 73 64 75  3 70 61  3 36 70\n",
      " 59  3 68 70 77 60 59  3 76 71 70 69  3 75 63 60  3 61 56 58 60  3 70 61\n",
      "  3 75 63 60  2], shape=(101,), dtype=int64)\n",
      "---------------------------------------\n",
      "Observe the conversion from ids to text:\n",
      "b' void; and darkness was upon\\r\\nthe face of the deep. And the Spirit of God moved upon the face of the\\r'\n",
      "---------------------------------------\n",
      "\n",
      "tf.Tensor(\n",
      "[ 1 78 56 75 60 73 74 14  2  1  2  1 17 26 19  3 30 69 59  3 36 70 59  3\n",
      " 74 56 64 59 12  3 41 60 75  3 75 63 60 73 60  3 57 60  3 67 64 62 63 75\n",
      " 26  3 56 69 59  3 75 63 60 73 60  3 78 56 74  3 67 64 62 63 75 14  2  1\n",
      "  2  1 17 26 20  3 30 69 59  3 36 70 59  3 74 56 78  3 75 63 60  3 67 64\n",
      " 62 63 75 12  3], shape=(101,), dtype=int64)\n",
      "---------------------------------------\n",
      "Observe the conversion from ids to text:\n",
      "b'\\nwaters.\\r\\n\\r\\n1:3 And God said, Let there be light: and there was light.\\r\\n\\r\\n1:4 And God saw the light, '\n",
      "---------------------------------------\n",
      "\n",
      "tf.Tensor(\n",
      "[75 63 56 75  3 64 75  3 78 56 74  3 62 70 70 59 26  3 56 69 59  3 36 70\n",
      " 59  3 59 64 77 64 59 60 59  3 75 63 60  3 67 64 62 63 75  2  1 61 73 70\n",
      " 68  3 75 63 60  3 59 56 73 66 69 60 74 74 14  2  1  2  1 17 26 21  3 30\n",
      " 69 59  3 36 70 59  3 58 56 67 67 60 59  3 75 63 60  3 67 64 62 63 75  3\n",
      " 33 56 80 12  3], shape=(101,), dtype=int64)\n",
      "---------------------------------------\n",
      "Observe the conversion from ids to text:\n",
      "b'that it was good: and God divided the light\\r\\nfrom the darkness.\\r\\n\\r\\n1:5 And God called the light Day, '\n",
      "---------------------------------------\n",
      "\n",
      "tf.Tensor(\n",
      "[56 69 59  3 75 63 60  3 59 56 73 66 69 60 74 74  3 63 60  3 58 56 67 67\n",
      " 60 59  3 43 64 62 63 75 14  2  1 30 69 59  3 75 63 60  3 60 77 60 69 64\n",
      " 69 62  3 56 69 59  3 75 63 60  3 68 70 73 69 64 69 62  3 78 60 73 60  3\n",
      " 75 63 60  3 61 64 73 74 75  3 59 56 80 14  2  1  2  1 17 26 22  3 30 69\n",
      " 59  3 36 70 59], shape=(101,), dtype=int64)\n",
      "---------------------------------------\n",
      "Observe the conversion from ids to text:\n",
      "b'and the darkness he called Night.\\r\\nAnd the evening and the morning were the first day.\\r\\n\\r\\n1:6 And God'\n",
      "---------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## notice how the above dataset tensor object iteratively returns characters\n",
    "## Here, we'll create a new dataset tensor object that returns a 'batch' of characters\n",
    "\n",
    "seq_length = 100 # feel free to vary this integer variable\n",
    "\n",
    "## Note: we added 1 to the seq_length because in the next code chunk, we'll use this object to create our X and y data.\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True) \n",
    "\n",
    "for seq in sequences.take(5): # feel free to vary this integer variable\n",
    "  print(seq)\n",
    "  print('---------------------------------------')\n",
    "  print('Observe the conversion from ids to text:')\n",
    "  print(ids_to_text(seq).numpy())\n",
    "  print(\"---------------------------------------\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this step, we'd have created a dataset tensor object that gives us (seq_length + 1) number of characters. In our example, that would be 100 + 1 = 101. Here's how we use this object to create our input sequences (X) and target sequences (y).\n",
    "\n",
    "For illustration purposes, let's assume our seq_length = 6 and our text is 'Federer'. In that case, our input sequence will be 'Federe' and the target sequence will be 'ederer'. So, we'll create a function that will return 2 output. The first output will be the first 100 (seq_length) number of characters (our X), whilst the second output will be the last 100 (seq_length) number of characters (our y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'1:1 In the beginning God created the heaven and the earth.\\r\\n\\r\\n1:2 And the earth was without form, an'\n",
      "Target: b':1 In the beginning God created the heaven and the earth.\\r\\n\\r\\n1:2 And the earth was without form, and'\n"
     ]
    }
   ],
   "source": [
    "## Create a function that will produce the X and y variables off a sequence of text, \n",
    "## regardless of the text's given length.\n",
    "\n",
    "def split_into_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "## use the .map method to create a dataset tensor object \n",
    "## that will always return our X and y outputs respectively when we call it\n",
    "dataset_xy = sequences.map(split_into_input_target)\n",
    "\n",
    "## view an example\n",
    "for input_example, target_example in dataset_xy.take(1):\n",
    "    print(\"Input :\", ids_to_text(input_example).numpy())\n",
    "    print(\"Target:\", ids_to_text(target_example).numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create Training Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "## Before we use our dataset, we'll need to shuffle the data and create batches.\n",
    "## Shuffling helps prevent our model from overfitting.\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset_xy\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)) # this allows later elements to be prepared while the current element is processed\n",
    "\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## size of our vocab - i.e. the number of unique characters in our dataset\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "## Our embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "## number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that builds our langage model architecture with the Sequential API\n",
    "# To keep this simple, we'll used 3 simple layers - embedding, GRU and dense. You can\n",
    "# replace GRU with LSTM.\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size\n",
    "                        ,embedding_dim\n",
    "                        ,batch_input_shape=(batch_size,None))) # the use of none gives us flexibility with the input's seqeunce length\n",
    "    model.add(GRU(rnn_units\n",
    "                  ,return_sequences=True # this returns y as the full sequences, rather than the last output\n",
    "                  ,stateful=True)) # This allows LSTMs to have longer context at training time\n",
    "    model.add(Dense(vocab_size)) # Notice how we are not using any softmax activation. \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size = vocab_size\n",
    "                    ,embedding_dim=embedding_dim\n",
    "                    ,rnn_units=rnn_units\n",
    "                    ,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           20992     \n",
      "                                                                 \n",
      " gru (GRU)                   (64, None, 1024)          3938304   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 82)            84050     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,043,346\n",
      "Trainable params: 4,043,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 82) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29, 65,  8, 40,  7, 26, 70, 33, 17, 40, 34,  5, 39,  8,  1, 14, 64,\n",
       "       10,  3, 20, 27, 49, 74, 75, 57, 72, 58, 26, 10, 25, 36, 68, 57,  3,\n",
       "       49, 33, 46, 18, 25, 42,  9, 47, 56, 55, 47, 67,  5, 40, 78, 24, 79,\n",
       "       21, 29, 29, 17, 64, 19, 19, 15, 29, 30, 17, 37, 36, 43, 16, 43, 34,\n",
       "        8,  7, 67, 53, 10,  8, 49, 63, 73, 47, 35, 68, 47, 51, 19, 34, 64,\n",
       "       62, 28, 71, 81, 48,  6, 24, 42, 75, 21, 10,  0, 34, 59, 29])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To get actual predictions, we will sample from the output distribution.\n",
    "## If we don't sample from an output distribution, it can cause the model to be stuck in a loop. \n",
    "## meaning, it will keep producing a repeated sequence of characters. Hence, we sample.\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 82)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.405335\n"
     ]
    }
   ],
   "source": [
    "## For our loss calculation, we'll use the 'sparse_categorical_crossentropy'\n",
    "## because the target labels are provided as integers. If they are one-hot representations,\n",
    "## then we'll use 'CategoricalCrossentropy'.\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "688/688 [==============================] - 2109s 3s/step - loss: 1.7533\n",
      "Epoch 2/20\n",
      "688/688 [==============================] - 3030s 4s/step - loss: 1.1778\n",
      "Epoch 3/20\n",
      "688/688 [==============================] - 2841s 4s/step - loss: 1.0850\n",
      "Epoch 4/20\n",
      "688/688 [==============================] - 4240s 6s/step - loss: 1.0392\n",
      "Epoch 5/20\n",
      "688/688 [==============================] - 2553s 4s/step - loss: 1.0081\n",
      "Epoch 6/20\n",
      "688/688 [==============================] - 1470s 2s/step - loss: 0.9836\n",
      "Epoch 7/20\n",
      "688/688 [==============================] - 1433s 2s/step - loss: 0.9639\n",
      "Epoch 8/20\n",
      "688/688 [==============================] - 2031s 3s/step - loss: 0.9466\n",
      "Epoch 9/20\n",
      "688/688 [==============================] - 2386s 3s/step - loss: 0.9323\n",
      "Epoch 10/20\n",
      "688/688 [==============================] - 1660s 2s/step - loss: 0.9200\n",
      "Epoch 11/20\n",
      "688/688 [==============================] - 1462s 2s/step - loss: 0.9093\n",
      "Epoch 12/20\n",
      "688/688 [==============================] - 1339s 2s/step - loss: 0.9004\n",
      "Epoch 13/20\n",
      "688/688 [==============================] - 1025s 1s/step - loss: 0.8938\n",
      "Epoch 14/20\n",
      "688/688 [==============================] - 879s 1s/step - loss: 0.8886\n",
      "Epoch 15/20\n",
      "688/688 [==============================] - 1190s 2s/step - loss: 0.8837\n",
      "Epoch 16/20\n",
      "688/688 [==============================] - 1197s 2s/step - loss: 0.8812\n",
      "Epoch 17/20\n",
      "688/688 [==============================] - 18748s 27s/step - loss: 0.8786\n",
      "Epoch 18/20\n",
      "688/688 [==============================] - 914s 1s/step - loss: 0.8778\n",
      "Epoch 19/20\n",
      "688/688 [==============================] - 1224s 2s/step - loss: 0.8778\n",
      "Epoch 20/20\n",
      "688/688 [==============================] - 1129s 2s/step - loss: 0.8787\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=20\n",
    "history = model.fit(dataset_subclassing, epochs=EPOCHS)#, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('sequential-bible-weights.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Text\n",
    "\n",
    "### 7.1. Create a new model with the saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the sake of this project, we don't need to produce a BATCH_SIZE worth of different generated texts. \n",
    "## Instead, we just need batch size of 1. But, the model has been built to produce the stated BATCH_SIZE.\n",
    "## Hence, we'll need to create a new model, then restore the weights that was saved.\n",
    "\n",
    "bible_text_generator = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "bible_text_generator.load_weights('sequential-bible-weights.h5')\n",
    "\n",
    "bible_text_generator.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (1, None, 256)            20992     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (1, None, 1024)           3938304   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (1, None, 82)             84050     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,043,346\n",
      "Trainable params: 4,043,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bible_text_generator.summary()\n",
    "\n",
    "## observe how index 0 of the output shape are now the batch size of 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Create a function that will generate a text for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To aid us in our text generation, we will write a function that will help us \n",
    "## generate text with the help of some starting characters.\n",
    "\n",
    "def generate_text(model, start_string, temperature = 1.0, prediction_length = 1000):  \n",
    "    \n",
    "    '''\n",
    "    start_string: Produce predictions from this starting string\n",
    "    temperature (default = 1): Ranges from 0 to 1, where 1 produces more diverse characters and 0 produces more conservative selections.\n",
    "    prediction_length (default = 1000): The length of characters to generate, excluding the length of the start_string\n",
    "    '''\n",
    "\n",
    "    input_eval = [ids_from_chars(s) for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "   \n",
    "    text_generated = []\n",
    "\n",
    "    model.reset_states() # resets the states of all layers in the model\n",
    "    for i in range(prediction_length):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(chars_from_ids(predicted_id))\n",
    "\n",
    "    return (start_string + tf.strings.join(text_generated).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jesus Christ.\n",
      "\n",
      "1:6 For thou viltest is the law and the earth, which is to the visitation after their sins.\n",
      "\n",
      "1:12 Who am I now the former the fourteenth day of the month, the\n",
      "end of uncircumcision is come to wife, and enquire of the LORD: and they\n",
      "shall turn the fitches at laws as it: 2:4 And declared the brethren of\n",
      "Israel, and gave me an altar and aside the graven image, which\n",
      "believe and know, that thou art my glory for ever.\n",
      "\n",
      "1:18 And Christ is the people of Israel, and said, Am not I to\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(bible_text_generator, start_string=u\"Jesus\", temperature = 0.8, prediction_length = 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For God so loved the world, and the firstborn of the\n",
      "LORD shall be ashamed of him.\n",
      "\n",
      "1:12 But the word of the LORD came unto me, saying, 18:2 Son of man,\n",
      "when the people which were sick for a sin offering, and the fathers of the people of the land shall be called\n",
      "the chief priests and the Pharisees and Paul, and said, O \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(bible_text_generator, start_string=u\"For God so loved the world\", temperature = 0.25, prediction_length = 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jesus stood and drink offering and\n",
      "offerings for the multitude of thorns, and beguilt an everlasting kingdom of God, and\n",
      "to break the sword of the LORD, and the Princes to pass through the land, and\n",
      "they gather them in bread, when he shall overcome thee.\n",
      "\n",
      "10:13 Wherefore if ye had unto thy fathers are the gospel of Christ, as\n",
      "I have made them also that have been as the stermass and upon meekness unto the law.\n",
      "\n",
      "11:15 And the woman which did contint the woman, and the holy\n",
      "and two weders of th\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(bible_text_generator, start_string=u\"Jesus\", temperature = 0.8, prediction_length = 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love the Lord with all your hearts, and will be the seven loaves and\n",
      "purposes, and all the people of the land of Moab, and the border of the\n",
      "children of Israel, and the strangers that sit in the midst of the seas, and the princes of the\n",
      "children of Israel, and the strangers that were slain by the sword, and be found in the midst of the\n",
      "season, and of the country of the Philistines, and to the children of Gad, and\n",
      "Arimathaea, and John, and Andrew sister Jesus Christ.\n",
      "\n",
      "1:11 For the Son of man is not able to do the wor\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(bible_text_generator, start_string=u\"Love the Lord with all your \", temperature = 0.2, prediction_length = 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love the Lord with all your hearts:\n",
      "for it is a sinner, and the heart of the LORD is the head of the gospel of Christ and his brother.\n",
      "\n",
      "2:1 When ye have the prophet in the land of Egypt, and they shall be\n",
      "the father of the Philistines, and the sons of Zebedee, and James, and John, and\n",
      "came to Jeremiah from the LORD, and t\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(bible_text_generator, start_string=u\"Love the Lord with all your \", temperature = 0.2, prediction_length = 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For God so loved the same words of the Lord, and he shall be as the\n",
      "head of the sanctuary: and the sun was sent and carried into the house of\n",
      "Eliashib the son of Jehoiakim king of Judah, saying, 3:2 The same is he that shall be desolate, and the spirit of the LORD of hosts is the heart of the covenant with the sword, \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(bible_text_generator, start_string=u\"For God so loved the \", temperature = 0.2, prediction_length = 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
